---
title: "Introduction to Representational Similarity Analysis"
author: "Chris Cox"
bibliography: rsa-references.bib
csl: apa.csl
---
---
nocite: |
    @Dujmovic2022, @DimsdaleZucker2018, @Kriegeskorte2008, @Nili2014, |
    @Diedrichsen2017, @Cai2019, @Kaniuth2022, @Friston2019, @Keshavarz2024
---


## What is Representational Similarity Analysis (RSA)?

There are many very good papers written about the logic, strengths, and weaknesses of RSA. It is worth digging into them. One of may many languishing projects writing another paper in this vein...

RSA boils down to a comparison of similarity or dissimilarity (i.e., distance) matrices. Similarity (and dissimilarity) matrices are *symmetric*, meaning that `x[i,j] == x[j,i]` is `TRUE` for all `i` and `j`. All values along the *diagonal* of a dissimilarity matrix (i.e., whenever `i==j`) will be zero. For similarity matrices useful for RSA, such as correlation matrices or cosine similarity matrices, the diagonal values will always be one.[^covariance]

[^covariance]: A covariance matrix is an example of a similarity matrix that does not have ones on the diagonal. Instead, it has the item variances on the diagonal.


### Exploring similarity and distance matrices

Let's load some packages and a selection of data packaged with R that to utilize for a demonstration:

```{r generate symmetric matrices}
library(dplyr, warn.conflicts = FALSE)
library(purrr)
library(tibble)
library(tidyr)
library(ggplot2)

data("mtcars")
x <-  mtcars[, c(1,3,4,5,6,7)]
knitr::kable(x, digits = 1)
```


I add a little noise to the data to reduce the similarity among items. Then I compute three matrices: a correlation _similarity_ matrix, a correlation _distance_ matrix (i.e., dissimilarity), and a Euclidean distance matrix.

```{r}
e <- matrix(
    rnorm(nrow(x) * ncol(x), sd = apply(x, 2, sd)),
    nrow = nrow(x),
    ncol = ncol(x),
    byrow = TRUE
)
matrices <- list(
    similarity = cor(t(x + e)),
    dissimilar = (2 - (cor(t(x + e)) + 1)) / 2,
    distance = as.matrix(dist(x + e))
)
```

To plot these matrices with `ggplot`, I will represent them as `tibble`s (which are just `data.frame`s with some conveniences baked in):

```{r matrix to tibble}
d <- map(matrices, ~ {
    .x |>
        as_tibble(rownames = "carA") |>
        pivot_longer(
            cols = -carA,
            names_to = "carB",
            values_to = "value"
        )
}) |>
    list_rbind(names_to = "metric") |>
    mutate(
        metric = factor(
            metric,
            levels = c("similarity", "dissimilar", "distance"),
            labels = c(
                "correlation (similarity)",
                "correlation distance",
                "Euclidean distance"
            )
        ),
        across(c(carA, carB), ~ factor(.x, levels = rownames(x)))
    )
```

I will also define a logical matrix that can be used as a filter to select the lower triangle of each of these matrices as:

```{r lower triangle}
lt <- lower.tri(matrices$similarity)
```

Now I will plot the correlation similarity and distance matrices:

```{r plot similarity/dissimilarity matrices}
d |>
    filter(metric != "Euclidean distance") |>
    ggplot(aes(x = carA, y = carB, fill = value)) +
        geom_raster() +
        scale_fill_gradient2() +
        theme(
            axis.title.y = element_blank(),
            axis.title.x = element_blank(),
            axis.text.x = element_blank(),
            axis.ticks.x = element_blank()
        ) +
        facet_wrap(vars(metric))
```

Notice that the two express the same patterns of similarity, but inverted and on different scales. Indeed, the correlation between the lower triangles of the two matrices is -1:

```{r correlation}
cor(matrices$similarity[lt], matrices$dissimilar[lt])
```

Distances cannot be negative, so 0 is the bottom of the scale for the correlation distance matrix.

Now I will plot the Euclidean distance:

```{r plot distance matrix}
d |>
    filter(metric == "Euclidean distance") |>
    ggplot(aes(x = carA, y = carB, fill = value)) +
        geom_raster() +
        scale_fill_gradient() +
        theme(
            axis.title.y = element_blank(),
            axis.title.x = element_blank(),
            axis.text.x = element_blank(),
            axis.ticks.x = element_blank()
        ) +
    facet_wrap(vars(metric))
```


::: {.callout-note}
Notice that the Euclidean distances among items do not convey the same structure as the matrices above:

```{r correlation matrix}
imap(matrices, ~ tibble(.y := .x[lt])) |>
    list_cbind() |>
    cor() |>
    knitr::kable(digits = 3)
```

Why is this? Reflect on the differences between how correlation and Euclidean distance are computed. In the following equations, $x$ and $y$ refer to different items like `Honda Civic` and `Fiat 128`, and the numbers iterate over features like `mpg` (miles per gallon), and `hp` (horsepower)).

$$
\textrm{Euclidean Dist.} = \sqrt{\sum(x_i - y_i)^2}
$$
$$
\textrm{Pearson's } r = \frac{\sum(x_i-\bar{x})(y_i-\bar{y})}{\sqrt{\sum(x_i - \bar{x})^2\sum(y_i-\bar{y})^2}}
$$

First, notice that the correlation subtracts the mean from each vector. While its not as obvious in the formulation above, we could also think about Pearson's $r$ as:

$$
\begin{align}
\textrm{Pearson's } r &= \frac{\sum z(x)_i-z(y)_i}{n-1} \\
z(a)_i &= \frac{a_i - \bar{a}}{s(a)} \\
s(a) &= \sqrt{\frac{\sum a_i - \bar{a}}{n-1}}
\end{align}
$$

In other words, each variable is also standardized so that the vectors being correlated both have standard deviation equal to one (i.e., "unit variance"). It is important to consider what structure and relationships are being emphasized by the choice of similarity or distance metric.

:::


While similarity and dissimilarity matrices have some important differences when considering relationships among items, when focusing on the mechanics of RSA they are interchangeable. From this point on, in the interest of brevity, I will write "similarity matrix" and not explicitly mention dissimilarity (or distance) matrices.


## First example

Let's simulate some data to make things more concrete. First, I will generate the "true structure": three independent dimensions of variability that characterize 100 items.

```{r E1 true signal}
x <- matrix(
    rnorm(300),
    nrow = 100,
    ncol = 3,
    dimnames = list(
        items = NULL,
        dimensions = NULL
    )
)

print(x[1:10, ])
```

::: {.callout-note}
How can three random vectors be "structure"? While it is true that there is no interesting topology to the 100 points, they are still 100 points in space, each with some similarity/distance relative to every other point. As long as these positions are systematic, in that different attempts to measure these points will result in them being positioned similarly in space, that's all we need in order to say there is structure.
:::

The true 3D-structure will be a part of the matrices to which we will apply RSA. First, we'll create some "MRI data" by:

    1. Replicating the columns
    2. Adding some noise on top of the true signal
    3. Appending some completely unrelated columns

```{r E1 fmri data}
noise_sd <- 1
e <- matrix(rnorm(300 * 10, sd = noise_sd), nrow = 100, ncol = 3 * 10)
xe <- matrix(c(x) + c(e), nrow = 100, ncol = 3 * 10)
E <- matrix(rnorm(300 * 10, sd = noise_sd), nrow = 100, ncol = 3 * 10)
X <- cbind(xe, E)

print(dim(X))
```

::: {.callout-warning}
In the code above, I leverage the facts that `R` will recycle the shorter vector to match the longer one (assuming the length of the longer one is multiple of the shorter one) and that matrices are populated one column at a time by default. For example:

```{r matrix columnwise population}
matrix(1:6, nrow = 3, ncol = 2)
```
:::

We'll say `X` is the "fMRI data". There are 60 voxels in total. The former 30 carry signal, and the latter 30 are completely unrelated to the true 3D structure. The first voxel is derived from the first dimension of the the true structure, the second is derived from the second dimension of the true structure, and likewise for the third. This pattern repeats 10 times.

I set the standard deviation of the noise to be 1, which is equal to the standard deviation all three dimensions of "signal". In other words, the signal to noise ratio is 1:1 for any signal-carrying voxel.

::: {.callout-idea}

We know the signal to noise ratio exactly since this is a simulation. But thinking about real data in terms of potential sources of variance is very helpful, even if you do not know what they all are.

:::

You might think that a situation where the noise has the same intensity as the signal as hopeless, but there are two reasons why it is not.

 1. We are interested in how this structure correlates with a target structure, which means that even if half of the variance we observe is completely unrelated to the target structure, half of the variance _is_ related to the target.
 2. Additionally, each of the three dimensions of signal is repeated across multiple voxels. This means that while the noise is uncorrelated between voxels, the signal is _correlated_ between voxels. This is true of this simulation, and also has some truth in many real datasets.
 
To highlight this second point, we will first compute the correlation matrix *among voxels*. 

```{r}
r <- cor(X)
dim(r)
```

Then we will decompose that correlation matrix into its eigenvalues and eigenvectors.

```{r}
e <- eigen(r)
str(e)
```

The eigenvectors are essentially principal components: they are all orthogonal.

::: {.callout-note}
Two vectors are orthogonal when they are perpendicular to each other. This is hard to envision in more than two dimensions. However, for any pair of orthogonal vectors, their _dot product_ is zero.

We know the vectors $<1, 0>$ and $<0, 1>$ are orthogonal.

```{r}
a <- c(x = 1, y = 0)
b <- c(x = 0, y = 1)
plot(0, type = "n", xlim = c(-2, 2), ylim = c(-2, 2), xlab = "", ylab = "")
abline(v = 0, h = 0)
arrows(
    x0 = c(0, 0),
    y0 = c(0, 0),
    x1 = c(a["x"], b["x"]),
    y1 = c(a["y"], b["y"]),
    lwd = 3
)

```

The dot product is the sum of elementwise products between the vectors:

```{r}
unname((a[1] * b[1]) + (a[2] * b[2]))
sum(a * b)
```

If those vectors are represented as matrices, we can formulate this slightly differently.

```{r}
A <- matrix(a, nrow = 2, ncol = 1) 
B <- matrix(b, nrow = 2, ncol = 1)
list(A = A, B = B)
```

Notice that each matrix is a "column vector": because it is a matrix the vector could be expressed as rows or columns, and that matters when doing operations with other matrices. When taking the dot product of two matrices, each row in the first matrix is multiplied with each column of the second matrix, and these products are summed. If we want to use matrix operations to take the dot product between these two matrices, we will need to transpose one or the other of them.

```{r}
t(A) %*% B
```

If we combine `A` and `B` into a single $2 \times 2$ matrix `C`, we can multiply it with itself.

```{r}
C <- cbind(A = A, B = B)
t(C) %*% C
```

This is the matrix cross product, and can be expressed in R simply as:

```{r}
crossprod(C)
```

We see this produces a symmetric matrix, indicating that A and B are orthogonal (because their dot products are zero).

This is all build up to showing that the cross product of the matrix of eigenvectors (each eigenvector is a column in this matrix) produces a matrix with ones along the diagonal and zeros everywhere else, proving that they are orthogonal. In the following, I compute the cross product of the first 10 eigenvectors to constrain the output, this is true for all pairs of vectors.

```{r}
crossprod(e$vectors[, 1:10]) |> knitr::kable(digits = 2)
```

:::

The eigenvectors are ordered from most to least "important", where importance is determined by the proportion of variance it explains in the correlation matrix. This "importance" is captured by the eigenvalues, which we can plot below:

```{r}
plot(e$values, xlab = "eigenvalues")
```

The first three eigenvalues clearly separate from the 57. A principal components analysis would show that the first 30 voxels load on these three components, while the latter 30 voxels do not.

In summary, we have a matrix `X` of simulated fMRI data, containing some structure

Now, we can define the target representational similarity matrix (RSM) as the pairwise correlations among items. The correlation between two items is defined as their shared variability in 3D space.

```{r }
y <- cor(t(x))
```

Just to drive home the consequence of this decision to use correlation, rather than say Euclidean distance, consider the following:

```{r }
cor_dist <- function(x) {
    r <- cor(x)
    (2 - (r + 1)) / 2
}
std_dist <- function(x) {
    d <- as.matrix(dist(x))
    d / max(d)
}
y <- rbind(
    A = c( 1,  3,  2),
    B = c(-3, -1, -2),
    C = c( 0,  1,  1)
)
cor_dist(t(y))
std_dist(y)

```

```{r }
#lt <- lower.tri(y)
#cor(dist(x[, 1]), cor(t(X[, seq(1, 30, by = 3)]))[lt])
#
#
#X1d <- map(seq(1, 30, by = 3), ~ c(dist(X[, .x]))) |>
#    do.call(rbind, args = _) |>
#    colMeans()
#
#cor(dist(x[, 1]), X1d)

```

RSA tests whether two similarity matrices are are more correlated than would be expected by chance given the number of observations in the lower triangle similarity among experimental items is reflected in the patterns of neural activity they are associated with [@Kriegeskorte2008].

## A short selection of papers on RSA
::: {#refs}
:::
